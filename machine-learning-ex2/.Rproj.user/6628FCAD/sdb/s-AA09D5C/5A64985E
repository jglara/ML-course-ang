{
    "collab_server" : "",
    "contents" : "library(ggplot2)\nlibrary(dplyr)\n\n\n\nsigmoid <- function(z) {\n  1 / (1 + exp(-z))\n}\n\ncostFunction <- function(X,y) {\n  function (theta) {\n    h <- sigmoid(as.matrix(X) %*% theta)\n    J <- -log(h)*y - log(1-h)*(1-y)\n    J <- mean(J[,])\n  }\n}\n\ngradient <- function(X,y) {\n  function (theta) {\n    h <- sigmoid(as.matrix(X) %*% theta)\n    apply(X * rep((h-y), length(X)), MARGIN = 2, mean)\n  }\n}\n\ndata <- read.csv(\"ex2/ex2data1.txt\", header = F)\ndf <- data.frame(intercept=1, exam1=data[,1],exam2=data[,2],admitted=data[,3])\nggplot(data = df) + geom_point(aes(x=exam1,y=exam2,color=admitted))\n\nX <- df %>% select(intercept,exam1,exam2)\ny <- df %>% select(admitted)\n\ninitial_theta <- rep(0,length(X))\n\ncf <- costFunction(X,y)(initial_theta)\ngrad <- gradient(X,y)(initial_theta)\n\ntest_theta <- c(-24,0.2, 0.2)\ncf <- costFunction(X,y)(test_theta)\ngrad <- gradient(X,y)(test_theta)\n\n#######\n\noptimRes <- optim(par=initial_theta, fn=costFunction(X,y), gr = gradient(X,y), method=\"BFGS\", control = list(maxit = 400))\n\ntheta <- optimRes$par\ncost <- optimRes$value\n\n# Print theta to screen\ncat(sprintf('Cost at theta found by optim: %f\\n', cost))\ncat(sprintf('theta: \\n'))\ncat(sprintf(' %f \\n', theta))\n\n# Plot Boundary\nggplot(data = df) + geom_point(aes(x=exam1,y=exam2,color=admitted)) + geom_abline(intercept = -theta[1]/theta[3], slope= -theta[2] / theta[3])\n\n# como hacerlo usando glm\nglm(data=df, admitted ~ exam1+exam2, family = binomial(link = logit))",
    "created" : 1513757886139.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2800219978",
    "id" : "5A64985E",
    "lastKnownWriteTime" : 1513767466,
    "last_content_update" : 1513767466966,
    "path" : "~/git/ML/machine-learning-ex2/ex2.R",
    "project_path" : "ex2.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}